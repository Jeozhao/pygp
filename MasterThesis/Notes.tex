\documentclass[11pt, a4paper]{article}

\usepackage{amsmath,amssymb}
\usepackage{MnSymbol}

\begin{document}

\section{Theory}
\label{sec:theory}

Allways try to fit the data: $p({\bf f|x}) = \mathcal N({\bf y|0,K_{xx}})$. While ${\bf y}$ are the noisy observations of $\bf f$

\subsection{Variational GP}
\label{sec:vargpp}
Try to sparsen the input matrix $X$ to be able to calculate the inverse of the covariance matrix more efficient. By introducing inducing inputs ${\bf u}$ we can rewrite the log marginal likelihood as
\begin{equation}
  \label{eq:gplvm_marginal}
  \log p({\bf y|x}) = \log \int p({\bf f|u})p({\bf y|f})p({\bf u})d{\bf f}d{\bf u}\enspace.
\end{equation}
We want to minimize the KL divergence between the variational distribution $q({\bf f, u})=p({\bf f|u})\phi({\bf u})$ (where $\phi({\bf u})$ is unconstrained variational distribution over the latent inputs $\bf u$) and the ``exact'' marginal likelihood. By introducing Jenson's inequality and some linear algebra this whole likelihood can be lower bounded by
\begin{equation}
  \label{eq:varGPlowerbound}
  LML({\bf u}) = \log[\mathcal N({\bf y|0,}\sigma^2{\bf I+Q})] - \frac 1{2\sigma^2}Tr({\bf K_{xx}-Q})\enspace, 
\end{equation}
where ${\bf Q=K_{xu}K_{uu}K_{ux}}$. This can now be optimized w.r.t. the ${\bf u}$s and even the number of inducing inputs.

\subsection{Bayesian GPLVM }
\label{sec:vargplvm}
Considering multidimensional timeseries $\{{\bf y}\}_n$ of $N$ samples with outputs $Y\in\mathbb R^{D\times N}$. Search for low dimensional input space $X\in\mathbb R^{Q\times N}$ and function $f(X)$ generating higher dimensional outputs $f(X) = Y$. We assume independence over latent features given latent variables, leading to
\begin{equation}
  \label{eq:vargplvmmodel}
  p(X,F,Y|{\bf t})=p(Y|F)p(F|X)p(X|{\bf t})=\prod_{d=1}^Dp({\bf y}_d|{\bf f}_d)p({\bf f}_d|X)\prod_{q=1}^Qp({\bf x}_q)|{\bf t})\enspace.
\end{equation}
It is intractable to marginalize out $X$, as it appears non-linearly in the GP prior over the function values ${\bf f}_d|X\tilde{}\mathcal{N}({\bf f}_d|{\bf 0,K}_{XX})$. Thus, marginalize out $F$ and learn latent variables $X$. This leads to the marginal likelihood over the outputs
\begin{equation}
  \label{eq:lmlvargplvm}
  p(Y|{\bf t}) = \int p(Y|F)p(F|X)p(X|{\bf t}) dXdF\enspace.
\end{equation}
This integral is analytically intractable and for that reason we introduce the variational part here. As in variational GP $M$ inducing inputs ${\bf u}_m\in\mathbb R^{D}$ and the variational density $q(F,U,X)=q(F|U,X)q(U)q(X))$ ($q(U)$ arbitrary variational and $q(X)=\prod_{q=1}^Q\mathcal N({\bf x}_q|{\bf \mu}_q,S_q)$) are introduced. Applying Jenson's inequality gives us a lower bound on the log marginal likelihood as
\begin{equation}
  \label{eq:lowerboundlml}
  p(Y|{\bf t})\geq \mathcal F(q) = \int q(X)\log (Y|F)p(F|X) dXdF - \text{KL}(q(X) || p(X|{\bf t})\enspace.
\end{equation}
Thus, as a summary, we now approximate the true posterior for the latent variables $p(X|Y)$ by the variational distribution $q(X)$. 

\subsection{Derivatives and technical details}
\label{sec:vargplvml:derv_techdet}
To be able to compute log marginal likelihood and derivates of it we need to compute $\psi$ statistics, because latent variables are given as distributions. 

\subsubsection{Squared exponential $\psi$ statistics}
\label{sec:SEpsi}
Computation of kernel:
\begin{equation}
  k(x_q,x_q') = \sigma^2_f \cdot \exp\left\{-\frac12\frac{||x_q - x_q'||^2}{\alpha_q}\right\}
\end{equation}
Computation of $\Psi$ statistics:
\begin{align*}
  \label{eq:psi_se}
  \psi_0 & = N\sigma_f^2\\
  (\Psi_1)_{nm} & = \sigma^2_f \prod_{q=1}^Q
  \frac{ e^{ - \frac{1}{2} \frac{ \alpha_q ({\boldsymbol{\mu}}_{nq}  -
        {\bf z}_{mq})^2}{\alpha_q S_{nq} + 1}}}
  {( \alpha_q S_{nq} + 1)^{\frac{1}{2}}} \\
  & = \sigma^2 \exp\left\{
    \sum_{q=1}^Q-\frac12 
    \left( \frac{\alpha_q({\boldsymbol{\mu}}_{nq} - {\bf z}_{mq})^2} {\alpha_qS_{nq}+1} 
      + \log(\alpha_qS_{nq}+1)
    \right)
  \right\}\\
  (\Psi^n_2)_{m m'} & = \sigma_f^4 
  \prod_{q=1}^Q \frac{ e^{-  \frac{\alpha_q (z_{mq} -
        z_{m'q})^2}{4} - \frac{\alpha_q \left(\mu_{nq} -
          \bar{z}_{q} \right)^2}{2 \alpha_q S_{nq} + 1}}}
  {(2 \alpha_q S_{nq} + 1)^{\frac{1}{2}}}\\
  & = \sigma_f^4\exp\left\{\sum_{q=1}^Q-\frac12\left(\alpha_q\left(\frac{(z_{mq}-z_{m'q})^2}2 + \frac{(\mu_{nq}-\bar z_q)^2}{\alpha_qS_{nq}+\frac12}\right) + \log(2\alpha_qS_{nq}+1)\right)\right\}
\end{align*}
Derivatives w.r.t hyperparameters $\sigma^2_f, \alpha_q$ and $\bf z$:
\begin{align*}
  \frac{\partial\psi_0}{\partial\sigma_f^2} & = N & \frac{\partial\psi_0}{\partial\alpha_q} & = 0\\
  \frac{\partial(\Psi_1)_{nm}}{\partial\sigma_f^2} & = \exp_{\Psi_1} 
  & \frac{\partial(\Psi_1)_{nm}}{\partial\alpha_q} & = (\Psi_1)_{nm}\frac{\partial\exp_{\Psi_1}}{\partial\alpha_q} \\
  \frac{\partial(\Psi_2^n)_{mm'}}{\partial\sigma_f^2} & = 2\sigma_f^2\exp_{\Psi_2} 
  & \frac{\partial(\Psi_2^n)_{mm'}}{\partial\alpha_q} & = (\Psi_2^n)_{mm'}\frac{\partial\exp_{\Psi_2}}{\partial\alpha_q} 
\end{align*}
\begin{align*}
    \frac{\partial\exp_{\Psi_1}}{\partial\alpha_q} &= -\frac12\left(\frac{(\boldsymbol{\mu}_{nq} - {\bf z}_{mq})^2\cdot(\alpha_qS_{nq}+1)-\alpha_q(\boldsymbol{\mu}_{nq} - {\bf z}_{mq})^2S_{nq}}{(\alpha_qS_{nq}+1)^2} + \frac{S_{nq}}{\alpha_qS_{nq}+1} \right)\\
    &= -\frac12\left(\frac{(\boldsymbol{\mu}_{nq} - {\bf z}_{mq})^2}{(\alpha_qS_{nq}+1)^2} + \frac{S_{nq}}{\alpha_qS_{nq}+1} \right)\\
  \frac{\partial\exp_{\Psi_2}}{\partial\alpha_q} &= -\frac12\left(\frac{(z_m-z_{m'})^2}2 + \frac{(\mu_{nq} - \bar z_q)^2(\alpha_qS_{nq}+1) - S_{nq}\alpha_1(\mu_{nq} - \bar z_q)^2}{(\alpha_qS_{nq}+\frac12)^2}\right)\\
  & - \frac{S_{nq}}{\alpha_qS_{nq}+1}
\end{align*}

\subsubsection{Linear $\Psi$ statistics}
\label{sec:linpsistat}
Kernel computation:
\begin{equation}
  k({\bf x,x}') = {\bf x}^T A {\bf x}'
\end{equation}
$\Psi$ statistic computation:
\begin{equation}
  \psi_0 = \text{tr}[A(\boldsymbol{\mu}_n\boldsymbol{\mu}^T_n+S_n)]
\end{equation}
\begin{equation}
  (\Psi_1)_{nm} = \boldsymbol{\mu}_n^TA{\bf z}_m
\end{equation}
\begin{equation}
  (\Psi_2)_{mm'} = {\bf z}^T_mA(\boldsymbol{\mu}_n\boldsymbol{\mu}_n^T+S_n)A{\bf z}_{m'}
\end{equation}
Derivatives w.r.t parameters:

\section{Applications}
\label{sec:applications}


\section{Plans}
\label{sec:plans}



\bibliography{MasterThesis}
\bibliographystyle{alpha}

\end{document}
